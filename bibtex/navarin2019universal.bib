@inproceedings{navarin2019universal,
author = {Navarin, Nicol√≤ and Van Tran, Dinh and Sperduti, Alessandro},
title = {Universal readout for graph convolutional neural networks},
booktitle = {2019 International Joint Conference on Neural Networks (IJCNN)},
year = {2019},
doi = {10.1109/IJCNN.2019.8852103},
user = {dinh},
pages = {1--7},
organization = {IEEE},
abstract = {
            Several machine learning problems can be naturally defined over graph data. Recently, many researchers have been focusing on the definition of neural networks for graphs. The core idea is to learn a hidden representation for the graph vertices, with a convolutive or recurrent mechanism. When considering discriminative tasks on graphs, such as classification or regression, one critical component to design is the readout function, i.e. the mapping from the set of vertex representations to a fixed-size vector (or the output). Different approaches have been presented in literature, but recent approaches tend to be complex, making the training of the whole network harder. In this paper, we frame the problem in the setting of learning over sets. Adopting recently proposed theorems over functions defined on sets, we propose a simple but powerful formulation for a readout layer that can encode or approximate arbitrarily well any continuous permutation-invariant function over sets. Experimental results on real-world graph datasets show that, compared to other approaches, the proposed readout architecture can improve the predictive performance of Graph Neural Networks while being computationally more efficient.}
}

